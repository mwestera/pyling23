# Python for Linguists 2023

## Coding Quest F: Feature structures

**_Finish [Section 13](../exercises/13_dictionary_basics.md) before attempting this quest._**


**F.1.** Dictionaries are useful for representing so-called **feature structures**, i.e., bundles of features with values. For instance, we can represent a vocabulary as a list of feature structures, one per lexical entry, as illustrated below. To get a feel for feature structures, add a couple more lexical entries and then write some code to extract, say, a random transitive verb from this vocabulary, or a random singular noun (with the help of `import random` and `random.choice`).

```python
vocabulary = [
    {'word': 'walk', 'category': 'verb', 'frame': 'intransitive'},
    {'word': 'see', 'category': 'verb', 'frame': 'transitive'},
    {'word': 'student', 'category': 'noun'},
    {'word': 'the', 'category': 'det'},
]
```


**F.2.** ⭐⭐ _[Optional, feel free to skip]_ One could perhaps debate the cognitive plausibility (and computational practicality...) of a lexicon containing both singular and plural forms of each noun (also, for the verbs the current approach basically assumes third person...). Feel free to pursue an alternative approach in this Coding Quest, e.g., you could store only the singular forms in the vocabulary and derive plural forms on the fly, but this will add some complexity.

**F.3.** ⭐ Define a function `build_DP` that generates a random determiner phrase, i.e., determiner + noun. It should ensure that the determiner and noun match in the grammatically relevant features (i.e., `number`). It returns a feature structure for the DP containing at least the constituent's `form` (the two words concatenated) and its `number` feature (inherited from the determiner and noun).

**F.4.** ⭐ Next, define a function `build_VP` that (unlike `build_DP`) takes `number` as an argument and generates a random verb phrase. It should sample a verb with the requested `number` feature, ensure that a transitive verb gets an object DP while an intransitive verb doesn't, and return a feature structure for the VP, containing at least the constituent's `form` (and any other features that you think may be relevant higher up in the syntax tree).

**F.5.** Now write a function `build_S` that builds a random sentence. It should call the aforementioned functions to retrieve a random DP and a random VP (passing in the `number` argument from the generated DP), and return a feature structure for the whole sentence. (For easy testing, consider implementing a loop that prints out, say, 20 random sentences generated by your grammar.)

**F.6.** ⭐⭐ _[Optional, feel free to skip.]_ If you dislike certain aspects of the approach thus far, e.g., the label `S`, the verb-external subject, or that the noun imposes its number on the verb (instead of either vice versa or top-down), feel free to change/complicate things based on your preferred theory of syntax.

**F.7.** ⭐⭐ Add _selectional restrictions_ to your verb entries (e.g., `'subject': 'animate'`), as well as relevant semantic information to your noun entries (e.g., `'animate': True`). (Extend your vocabulary if necessary to have some diversity in this regard.) Now, try to minimally change the functions in order for the verb's selectional restrictions to constrain the generated subject NP.

**F.8.** ⭐ Conceptually (no programming required): For the generated language to seem more 'human-like', one thing we could do is adjust the relative probabilities with which we sample the various lexical items (some should be more probable than others). Can you try to fill in some possible details of this idea? Where might we get suitable probabilities from? Could we perhaps make the probability of choosing one word depend on the words already chosen (why? how?)?



